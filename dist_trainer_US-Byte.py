# -*- coding: utf-8 -*-
from __future__ import print_function
import time
import torch
import numpy as np
import argparse, os, sys
import settings
import utils
import logging

from dl_trainer import DLTrainer, _support_datasets, _support_dnns
import torch.backends.cudnn as cudnn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data.distributed
from torchvision import models
import horovod.torch as hvd
from profiling import benchmark
import timeit
import math
import numpy as np
from mpi4py import MPI
comm = MPI.COMM_WORLD
writer = None

from horovod.torch.mpi_ops import allreduce_async_
from horovod.torch.mpi_ops import allgather_async
from horovod.torch.mpi_ops import broadcast_async_
from horovod.torch.mpi_ops import synchronize
from horovod.torch.mpi_ops import size, local_size, rank, local_rank
from horovod.torch.mpi_ops import init, broadcast
from profiling import CommunicationProfiler
from sklearn.linear_model import LinearRegression

from settings import logger, formatter

os.environ['HOROVOD_FUSION_THRESHOLD'] = '0'
# os.environ['HOROVOD_CACHE_CAPACITY'] = '0'
# os.environ['HOROVOD_MPI_THREADS_DISABLE'] = '1'
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"   # see issue #152
# os.environ["CUDA_VISIBLE_DEVICES"]="6,7"


def writeFile_add(filename, data):
    file_handle = open(filename, mode='a')
    file_handle.write(data)
    file_handle.close()

def Get_a_and_b(nworkers, S_p):
    if nworkers == 8:
        a = 1318.6 * 1e-06  # s
        b = 2.406 * 1e-10
    elif nworkers == 4:
        a = 948.98 * 1e-06  # s
        b = 1.541e-10
    elif nworkers == 2:
        a = 556.6 * 1e-06  # s
        b = 0.352e-10
    return a, b

def Get_t_f(dnn):
    if dnn == 'vgg16':
        t_f = [8.216717116632254e-08, 2.2185136214907084e-06, 8.216717116632254e-08, 4.732829059180178e-05, 1.6433434233264508e-07, 9.465658118360356e-05, 1.6433434233264508e-07, 0.00018931316236720712, 3.2866868466529015e-07, 0.00037862632473441423, 3.2866868466529015e-07, 0.0007572526494688285, 3.2866868466529015e-07, 0.0007572526494688285, 6.573373693305803e-07, 0.001514505298937657, 6.573373693305803e-07, 0.003029010597875314, 6.573373693305803e-07, 0.003029010597875314, 6.573373693305803e-07, 0.003029010597875314, 6.573373693305803e-07, 0.003029010597875314, 6.573373693305803e-07, 0.003029010597875314, 0.13193023937412476, 5.2586989546446424e-06, 0.021539630918224455, 5.2586989546446424e-06, 0.005258698954644642, 1.2838620494737896e-06]
    elif dnn == 'resnet50':
        t_f = [3.038555275119584e-05, 2.0670444048432542e-07, 2.0670444048432542e-07, 1.3229084190996827e-05, 2.0670444048432542e-07, 2.0670444048432542e-07, 0.00011906175771897145, 2.0670444048432542e-07, 2.0670444048432542e-07, 5.291633676398731e-05, 8.268177619373017e-07, 8.268177619373017e-07, 5.291633676398731e-05, 8.268177619373017e-07, 8.268177619373017e-07, 5.291633676398731e-05, 2.0670444048432542e-07, 2.0670444048432542e-07, 0.00011906175771897145, 2.0670444048432542e-07, 2.0670444048432542e-07, 5.291633676398731e-05, 8.268177619373017e-07, 8.268177619373017e-07, 5.291633676398731e-05, 2.0670444048432542e-07, 2.0670444048432542e-07, 0.00011906175771897145, 2.0670444048432542e-07, 2.0670444048432542e-07, 5.291633676398731e-05, 8.268177619373017e-07, 8.268177619373017e-07, 0.00010583267352797462, 4.1340888096865085e-07, 4.1340888096865085e-07, 0.0004762470308758858, 4.1340888096865085e-07, 4.1340888096865085e-07, 0.00021166534705594923, 1.6536355238746034e-06, 1.6536355238746034e-06, 0.00042333069411189847, 1.6536355238746034e-06, 1.6536355238746034e-06, 0.00021166534705594923, 4.1340888096865085e-07, 4.1340888096865085e-07, 0.0004762470308758858, 4.1340888096865085e-07, 4.1340888096865085e-07, 0.00021166534705594923, 1.6536355238746034e-06, 1.6536355238746034e-06, 0.00021166534705594923, 4.1340888096865085e-07, 4.1340888096865085e-07, 0.0004762470308758858, 4.1340888096865085e-07, 4.1340888096865085e-07, 0.00021166534705594923, 1.6536355238746034e-06, 1.6536355238746034e-06, 0.00021166534705594923, 4.1340888096865085e-07, 4.1340888096865085e-07, 0.0004762470308758858, 4.1340888096865085e-07, 4.1340888096865085e-07, 0.00021166534705594923, 1.6536355238746034e-06, 1.6536355238746034e-06, 0.00042333069411189847, 8.268177619373017e-07, 8.268177619373017e-07, 0.0019049881235035432, 8.268177619373017e-07, 8.268177619373017e-07, 0.0008466613882237969, 3.3072710477492068e-06, 3.3072710477492068e-06, 0.0016933227764475939, 3.3072710477492068e-06, 3.3072710477492068e-06, 0.0008466613882237969, 8.268177619373017e-07, 8.268177619373017e-07, 0.0019049881235035432, 8.268177619373017e-07, 8.268177619373017e-07, 0.0008466613882237969, 3.3072710477492068e-06, 3.3072710477492068e-06, 0.0008466613882237969, 8.268177619373017e-07, 8.268177619373017e-07, 0.0019049881235035432, 8.268177619373017e-07, 8.268177619373017e-07, 0.0008466613882237969, 3.3072710477492068e-06, 3.3072710477492068e-06, 0.0008466613882237969, 8.268177619373017e-07, 8.268177619373017e-07, 0.0019049881235035432, 8.268177619373017e-07, 8.268177619373017e-07, 0.0008466613882237969, 3.3072710477492068e-06, 3.3072710477492068e-06, 0.0008466613882237969, 8.268177619373017e-07, 8.268177619373017e-07, 0.0019049881235035432, 8.268177619373017e-07, 8.268177619373017e-07, 0.0008466613882237969, 3.3072710477492068e-06, 3.3072710477492068e-06, 0.0008466613882237969, 8.268177619373017e-07, 8.268177619373017e-07, 0.0019049881235035432, 8.268177619373017e-07, 8.268177619373017e-07, 0.0008466613882237969, 3.3072710477492068e-06, 3.3072710477492068e-06, 0.0016933227764475939, 1.6536355238746034e-06, 1.6536355238746034e-06, 0.007619952494014173, 1.6536355238746034e-06, 1.6536355238746034e-06, 0.0033866455528951877, 6.6145420954984135e-06, 6.6145420954984135e-06, 0.0067732911057903755, 6.6145420954984135e-06, 6.6145420954984135e-06, 0.0033866455528951877, 1.6536355238746034e-06, 1.6536355238746034e-06, 0.007619952494014173, 1.6536355238746034e-06, 1.6536355238746034e-06, 0.0033866455528951877, 6.6145420954984135e-06, 6.6145420954984135e-06, 0.0033866455528951877, 1.6536355238746034e-06, 1.6536355238746034e-06, 0.007619952494014173, 1.6536355238746034e-06, 1.6536355238746034e-06, 0.0033866455528951877, 6.6145420954984135e-06, 6.6145420954984135e-06, 0.006614542095498413, 3.2297568825675846e-06]
    elif dnn == 'resnet152':
        t_f = [5.057966872055545e-05, 3.440793790513976e-07, 3.440793790513976e-07, 2.2021080259289447e-05, 3.440793790513976e-07, 3.440793790513976e-07, 0.00019818972233360502, 3.440793790513976e-07, 3.440793790513976e-07, 8.808432103715779e-05, 1.3763175162055904e-06, 1.3763175162055904e-06, 8.808432103715779e-05, 1.3763175162055904e-06, 1.3763175162055904e-06, 8.808432103715779e-05, 3.440793790513976e-07, 3.440793790513976e-07, 0.00019818972233360502, 3.440793790513976e-07, 3.440793790513976e-07, 8.808432103715779e-05, 1.3763175162055904e-06, 1.3763175162055904e-06, 8.808432103715779e-05, 3.440793790513976e-07, 3.440793790513976e-07, 0.00019818972233360502, 3.440793790513976e-07, 3.440793790513976e-07, 8.808432103715779e-05, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.00017616864207431557, 6.881587581027952e-07, 6.881587581027952e-07, 0.0007927588893344201, 6.881587581027952e-07, 6.881587581027952e-07, 0.00035233728414863115, 2.752635032411181e-06, 2.752635032411181e-06, 0.0007046745682972623, 2.752635032411181e-06, 2.752635032411181e-06, 0.00035233728414863115, 6.881587581027952e-07, 6.881587581027952e-07, 0.0007927588893344201, 6.881587581027952e-07, 6.881587581027952e-07, 0.00035233728414863115, 2.752635032411181e-06, 2.752635032411181e-06, 0.00035233728414863115, 6.881587581027952e-07, 6.881587581027952e-07, 0.0007927588893344201, 6.881587581027952e-07, 6.881587581027952e-07, 0.00035233728414863115, 2.752635032411181e-06, 2.752635032411181e-06, 0.00035233728414863115, 6.881587581027952e-07, 6.881587581027952e-07, 0.0007927588893344201, 6.881587581027952e-07, 6.881587581027952e-07, 0.00035233728414863115, 2.752635032411181e-06, 2.752635032411181e-06, 0.00035233728414863115, 6.881587581027952e-07, 6.881587581027952e-07, 0.0007927588893344201, 6.881587581027952e-07, 6.881587581027952e-07, 0.00035233728414863115, 2.752635032411181e-06, 2.752635032411181e-06, 0.00035233728414863115, 6.881587581027952e-07, 6.881587581027952e-07, 0.0007927588893344201, 6.881587581027952e-07, 6.881587581027952e-07, 0.00035233728414863115, 2.752635032411181e-06, 2.752635032411181e-06, 0.00035233728414863115, 6.881587581027952e-07, 6.881587581027952e-07, 0.0007927588893344201, 6.881587581027952e-07, 6.881587581027952e-07, 0.00035233728414863115, 2.752635032411181e-06, 2.752635032411181e-06, 0.00035233728414863115, 6.881587581027952e-07, 6.881587581027952e-07, 0.0007927588893344201, 6.881587581027952e-07, 6.881587581027952e-07, 0.00035233728414863115, 2.752635032411181e-06, 2.752635032411181e-06, 0.0007046745682972623, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.002818698273189049, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.0014093491365945246, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0031710355573376803, 1.3763175162055904e-06, 1.3763175162055904e-06, 0.0014093491365945246, 5.505270064822362e-06, 5.505270064822362e-06, 0.002818698273189049, 2.752635032411181e-06, 2.752635032411181e-06, 0.012684142229350721, 2.752635032411181e-06, 2.752635032411181e-06, 0.005637396546378098, 1.1010540129644723e-05, 1.1010540129644723e-05, 0.011274793092756197, 1.1010540129644723e-05, 1.1010540129644723e-05, 0.005637396546378098, 2.752635032411181e-06, 2.752635032411181e-06, 0.012684142229350721, 2.752635032411181e-06, 2.752635032411181e-06, 0.005637396546378098, 1.1010540129644723e-05, 1.1010540129644723e-05, 0.005637396546378098, 2.752635032411181e-06, 2.752635032411181e-06, 0.012684142229350721, 2.752635032411181e-06, 2.752635032411181e-06, 0.005637396546378098, 1.1010540129644723e-05, 1.1010540129644723e-05, 0.011010540129644723, 5.376240297678087e-06]
    elif dnn == 'mobilenetv2':
        t_f = [9.462223332614102e-07, 2.838666999784231e-06, 9.462223332614102e-07, 9.462223332614102e-07, 9.462223332614102e-07, 3.0279114664365126e-05, 9.462223332614102e-07, 9.462223332614102e-07, 9.462223332614102e-07, 8.516000999352693e-06, 9.462223332614102e-07, 9.462223332614102e-07, 4.731111666307051e-07, 1.5139557332182563e-05, 4.731111666307051e-07, 4.731111666307051e-07, 2.838666999784231e-06, 4.541867199654769e-05, 2.838666999784231e-06, 2.838666999784231e-06, 2.838666999784231e-06, 2.5548002998058077e-05, 2.838666999784231e-06, 2.838666999784231e-06, 7.096667499460577e-07, 6.812800799482154e-05, 7.096667499460577e-07, 7.096667499460577e-07, 4.2580004996763464e-06, 0.00010219201199223231, 4.2580004996763464e-06, 4.2580004996763464e-06, 4.2580004996763464e-06, 3.8322004497087114e-05, 4.2580004996763464e-06, 4.2580004996763464e-06, 7.096667499460577e-07, 0.00010219201199223231, 7.096667499460577e-07, 7.096667499460577e-07, 4.2580004996763464e-06, 0.00010219201199223231, 4.2580004996763464e-06, 4.2580004996763464e-06, 4.2580004996763464e-06, 3.8322004497087114e-05, 4.2580004996763464e-06, 4.2580004996763464e-06, 9.462223332614102e-07, 0.00013625601598964309, 9.462223332614102e-07, 9.462223332614102e-07, 5.677333999568462e-06, 0.00018167468798619077, 5.677333999568462e-06, 5.677333999568462e-06, 5.677333999568462e-06, 5.1096005996116154e-05, 5.677333999568462e-06, 5.677333999568462e-06, 9.462223332614102e-07, 0.00018167468798619077, 9.462223332614102e-07, 9.462223332614102e-07, 5.677333999568462e-06, 0.00018167468798619077, 5.677333999568462e-06, 5.677333999568462e-06, 5.677333999568462e-06, 5.1096005996116154e-05, 5.677333999568462e-06, 5.677333999568462e-06, 9.462223332614102e-07, 0.00018167468798619077, 9.462223332614102e-07, 9.462223332614102e-07, 5.677333999568462e-06, 0.00018167468798619077, 5.677333999568462e-06, 5.677333999568462e-06, 5.677333999568462e-06, 5.1096005996116154e-05, 5.677333999568462e-06, 5.677333999568462e-06, 1.8924446665228204e-06, 0.00036334937597238154, 1.8924446665228204e-06, 1.8924446665228204e-06, 1.1354667999136923e-05, 0.0007266987519447631, 1.1354667999136923e-05, 1.1354667999136923e-05, 1.1354667999136923e-05, 0.00010219201199223231, 1.1354667999136923e-05, 1.1354667999136923e-05, 1.8924446665228204e-06, 0.0007266987519447631, 1.8924446665228204e-06, 1.8924446665228204e-06, 1.1354667999136923e-05, 0.0007266987519447631, 1.1354667999136923e-05, 1.1354667999136923e-05, 1.1354667999136923e-05, 0.00010219201199223231, 1.1354667999136923e-05, 1.1354667999136923e-05, 1.8924446665228204e-06, 0.0007266987519447631, 1.8924446665228204e-06, 1.8924446665228204e-06, 1.1354667999136923e-05, 0.0007266987519447631, 1.1354667999136923e-05, 1.1354667999136923e-05, 1.1354667999136923e-05, 0.00010219201199223231, 1.1354667999136923e-05, 1.1354667999136923e-05, 1.8924446665228204e-06, 0.0007266987519447631, 1.8924446665228204e-06, 1.8924446665228204e-06, 1.1354667999136923e-05, 0.0007266987519447631, 1.1354667999136923e-05, 1.1354667999136923e-05, 1.1354667999136923e-05, 0.00010219201199223231, 1.1354667999136923e-05, 1.1354667999136923e-05, 2.838666999784231e-06, 0.0010900481279171447, 2.838666999784231e-06, 2.838666999784231e-06, 1.7032001998705386e-05, 0.001635072191875717, 1.7032001998705386e-05, 1.7032001998705386e-05, 1.7032001998705386e-05, 0.00015328801798834845, 1.7032001998705386e-05, 1.7032001998705386e-05, 2.838666999784231e-06, 0.001635072191875717, 2.838666999784231e-06, 2.838666999784231e-06, 1.7032001998705386e-05, 0.001635072191875717, 1.7032001998705386e-05, 1.7032001998705386e-05, 1.7032001998705386e-05, 0.00015328801798834845, 1.7032001998705386e-05, 1.7032001998705386e-05, 2.838666999784231e-06, 0.001635072191875717, 2.838666999784231e-06, 2.838666999784231e-06, 1.7032001998705386e-05, 0.001635072191875717, 1.7032001998705386e-05, 1.7032001998705386e-05, 1.7032001998705386e-05, 0.00015328801798834845, 1.7032001998705386e-05, 1.7032001998705386e-05, 4.731111666307051e-06, 0.0027251203197928616, 4.731111666307051e-06, 4.731111666307051e-06, 2.838666999784231e-05, 0.00454186719965477, 2.838666999784231e-05, 2.838666999784231e-05, 2.838666999784231e-05, 0.0002554800299805808, 2.838666999784231e-05, 2.838666999784231e-05, 4.731111666307051e-06, 0.00454186719965477, 4.731111666307051e-06, 4.731111666307051e-06, 2.838666999784231e-05, 0.00454186719965477, 2.838666999784231e-05, 2.838666999784231e-05, 2.838666999784231e-05, 0.0002554800299805808, 2.838666999784231e-05, 2.838666999784231e-05, 4.731111666307051e-06, 0.00454186719965477, 4.731111666307051e-06, 4.731111666307051e-06, 2.838666999784231e-05, 0.00454186719965477, 2.838666999784231e-05, 2.838666999784231e-05, 2.838666999784231e-05, 0.0002554800299805808, 2.838666999784231e-05, 2.838666999784231e-05, 9.462223332614102e-06, 0.00908373439930954, 9.462223332614102e-06, 9.462223332614102e-06, 3.784889333045641e-05, 0.012111645865746052, 3.784889333045641e-05, 3.784889333045641e-05, 2.956944791441907e-05, 0.037848893330456414]
    elif dnn == 'densenet201':
        t_f = [0.00023102979756897294, 1.5716312759794079e-06, 1.5716312759794079e-06, 1.5716312759794079e-06, 1.5716312759794079e-06, 0.0002011688033253642, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.357446913969112e-06, 2.357446913969112e-06, 0.0003017532049880463, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0004023376066507284, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.92907818994852e-06, 3.92907818994852e-06, 0.0005029220083134106, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.714893827938224e-06, 4.714893827938224e-06, 0.0006035064099760926, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 5.500709465927928e-06, 5.500709465927928e-06, 0.0007040908116387747, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 6.2865251039176314e-06, 6.2865251039176314e-06, 0.0008046752133014568, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0004023376066507284, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.92907818994852e-06, 3.92907818994852e-06, 0.0005029220083134106, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.714893827938224e-06, 4.714893827938224e-06, 0.0006035064099760926, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 5.500709465927928e-06, 5.500709465927928e-06, 0.0007040908116387747, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 6.2865251039176314e-06, 6.2865251039176314e-06, 0.0008046752133014568, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 7.072340741907335e-06, 7.072340741907335e-06, 0.0009052596149641389, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 7.85815637989704e-06, 7.85815637989704e-06, 0.0010058440166268211, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 8.643972017886745e-06, 8.643972017886745e-06, 0.0011064284182895033, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 9.429787655876448e-06, 9.429787655876448e-06, 0.0012070128199521853, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.0215603293866152e-05, 1.0215603293866152e-05, 0.0013075972216148675, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.1001418931855855e-05, 1.1001418931855855e-05, 0.0014081816232775495, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.1787234569845558e-05, 1.1787234569845558e-05, 0.0015087660249402314, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.2573050207835263e-05, 1.2573050207835263e-05, 0.0032187008532058273, 6.2865251039176314e-06, 6.2865251039176314e-06, 0.0008046752133014568, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 7.072340741907335e-06, 7.072340741907335e-06, 0.0009052596149641389, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 7.85815637989704e-06, 7.85815637989704e-06, 0.0010058440166268211, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 8.643972017886745e-06, 8.643972017886745e-06, 0.0011064284182895033, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 9.429787655876448e-06, 9.429787655876448e-06, 0.0012070128199521853, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.0215603293866152e-05, 1.0215603293866152e-05, 0.0013075972216148675, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.1001418931855855e-05, 1.1001418931855855e-05, 0.0014081816232775495, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.1787234569845558e-05, 1.1787234569845558e-05, 0.0015087660249402314, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.2573050207835263e-05, 1.2573050207835263e-05, 0.0016093504266029136, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.3358865845824968e-05, 1.3358865845824968e-05, 0.0017099348282655958, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.414468148381467e-05, 1.414468148381467e-05, 0.0018105192299282778, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.4930497121804375e-05, 1.4930497121804375e-05, 0.00191110363159096, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.571631275979408e-05, 1.571631275979408e-05, 0.0020116880332536422, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.650212839778378e-05, 1.650212839778378e-05, 0.002112272434916324, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.728794403577349e-05, 1.728794403577349e-05, 0.0022128568365790066, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.807375967376319e-05, 1.807375967376319e-05, 0.0023134412382416884, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.8859575311752895e-05, 1.8859575311752895e-05, 0.0024140256399043706, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 1.96453909497426e-05, 1.96453909497426e-05, 0.0025146100415670528, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.0431206587732304e-05, 2.0431206587732304e-05, 0.002615194443229735, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.1217022225722006e-05, 2.1217022225722006e-05, 0.0027157788448924167, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.200283786371171e-05, 2.200283786371171e-05, 0.002816363246555099, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.2788653501701415e-05, 2.2788653501701415e-05, 0.002916947648217781, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.3574469139691116e-05, 2.3574469139691116e-05, 0.003017532049880463, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.4360284777680824e-05, 2.4360284777680824e-05, 0.0031181164515431455, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.5146100415670526e-05, 2.5146100415670526e-05, 0.0032187008532058273, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.593191605366023e-05, 2.593191605366023e-05, 0.0033192852548685095, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.6717731691649935e-05, 2.6717731691649935e-05, 0.0034198696565311917, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.750354732963964e-05, 2.750354732963964e-05, 0.003520454058193874, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.828936296762934e-05, 2.828936296762934e-05, 0.0036210384598565556, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.9075178605619046e-05, 2.9075178605619046e-05, 0.003721622861519238, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.986099424360875e-05, 2.986099424360875e-05, 0.00382220726318192, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.0646809881598455e-05, 3.0646809881598455e-05, 0.003922791664844602, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.143262551958816e-05, 3.143262551958816e-05, 0.0040233760665072844, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.2218441157577864e-05, 3.2218441157577864e-05, 0.004123960468169967, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.300425679556756e-05, 3.300425679556756e-05, 0.004224544869832648, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.3790072433557274e-05, 3.3790072433557274e-05, 0.004325129271495331, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.457588807154698e-05, 3.457588807154698e-05, 0.004425713673158013, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.5361703709536676e-05, 3.5361703709536676e-05, 0.0045262980748206946, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.614751934752638e-05, 3.614751934752638e-05, 0.004626882476483377, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.6933334985516086e-05, 3.6933334985516086e-05, 0.004727466878146059, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.771915062350579e-05, 3.771915062350579e-05, 0.004828051279808741, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.8504966261495495e-05, 3.8504966261495495e-05, 0.004928635681471423, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.92907818994852e-05, 3.92907818994852e-05, 0.0050292200831341055, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.00765975374749e-05, 4.00765975374749e-05, 0.005129804484796787, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.086241317546461e-05, 4.086241317546461e-05, 0.00523038888645947, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.1648228813454314e-05, 4.1648228813454314e-05, 0.005330973288122152, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.243404445144401e-05, 4.243404445144401e-05, 0.0054315576897848335, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.3219860089433716e-05, 4.3219860089433716e-05, 0.005532142091447516, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.400567572742342e-05, 4.400567572742342e-05, 0.03942908545177139, 2.200283786371171e-05, 2.200283786371171e-05, 0.002816363246555099, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.2788653501701415e-05, 2.2788653501701415e-05, 0.002916947648217781, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.3574469139691116e-05, 2.3574469139691116e-05, 0.003017532049880463, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.4360284777680824e-05, 2.4360284777680824e-05, 0.0031181164515431455, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.5146100415670526e-05, 2.5146100415670526e-05, 0.0032187008532058273, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.593191605366023e-05, 2.593191605366023e-05, 0.0033192852548685095, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.6717731691649935e-05, 2.6717731691649935e-05, 0.0034198696565311917, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.750354732963964e-05, 2.750354732963964e-05, 0.003520454058193874, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.828936296762934e-05, 2.828936296762934e-05, 0.0036210384598565556, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.9075178605619046e-05, 2.9075178605619046e-05, 0.003721622861519238, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 2.986099424360875e-05, 2.986099424360875e-05, 0.00382220726318192, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.0646809881598455e-05, 3.0646809881598455e-05, 0.003922791664844602, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.143262551958816e-05, 3.143262551958816e-05, 0.0040233760665072844, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.2218441157577864e-05, 3.2218441157577864e-05, 0.004123960468169967, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.300425679556756e-05, 3.300425679556756e-05, 0.004224544869832648, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.3790072433557274e-05, 3.3790072433557274e-05, 0.004325129271495331, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.457588807154698e-05, 3.457588807154698e-05, 0.004425713673158013, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.5361703709536676e-05, 3.5361703709536676e-05, 0.0045262980748206946, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.614751934752638e-05, 3.614751934752638e-05, 0.004626882476483377, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.6933334985516086e-05, 3.6933334985516086e-05, 0.004727466878146059, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.771915062350579e-05, 3.771915062350579e-05, 0.004828051279808741, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.8504966261495495e-05, 3.8504966261495495e-05, 0.004928635681471423, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 3.92907818994852e-05, 3.92907818994852e-05, 0.0050292200831341055, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.00765975374749e-05, 4.00765975374749e-05, 0.005129804484796787, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.086241317546461e-05, 4.086241317546461e-05, 0.00523038888645947, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.1648228813454314e-05, 4.1648228813454314e-05, 0.005330973288122152, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.243404445144401e-05, 4.243404445144401e-05, 0.0054315576897848335, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.3219860089433716e-05, 4.3219860089433716e-05, 0.005532142091447516, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.400567572742342e-05, 4.400567572742342e-05, 0.005632726493110198, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.4791491365413125e-05, 4.4791491365413125e-05, 0.00573331089477288, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.557730700340283e-05, 4.557730700340283e-05, 0.005833895296435562, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.6363122641392535e-05, 4.6363122641392535e-05, 0.0059344796980982445, 3.1432625519588157e-06, 3.1432625519588157e-06, 0.0009052596149641389, 4.714893827938223e-05, 4.714893827938223e-05, 0.047148938279382234, 2.455673868717825e-05]
    elif dnn == 'inceptionv3':
        t_f = [0.0056659253997078645, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.0032376716569759226, 4.215718303354066e-06, 4.215718303354066e-06, 0.004721604499756554, 4.215718303354066e-06, 4.215718303354066e-06, 0.0056659253997078645, 5.058861964024879e-06, 5.058861964024879e-06, 0.0032376716569759226, 4.215718303354066e-06, 4.215718303354066e-06, 0.004721604499756554, 4.215718303354066e-06, 4.215718303354066e-06, 0.004721604499756554, 4.215718303354066e-06, 4.215718303354066e-06, 0.004721604499756554, 4.215718303354066e-06, 4.215718303354066e-06, 0.0056659253997078645, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.006799110479649437, 5.058861964024879e-06, 5.058861964024879e-06, 0.006799110479649437, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.006799110479649437, 5.058861964024879e-06, 5.058861964024879e-06, 0.006799110479649437, 5.058861964024879e-06, 5.058861964024879e-06, 0.006799110479649437, 5.058861964024879e-06, 5.058861964024879e-06, 0.006799110479649437, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.002590137325580738, 3.3725746426832526e-06, 3.3725746426832526e-06, 0.06475343313951845, 2.0235447856099515e-05, 2.0235447856099515e-05, 0.020235447856099514, 2.634823939596291e-05, 2.2764878838111955e-05, 8.431436606708131e-07, 8.431436606708131e-07, 0.00024282537427319418, 8.431436606708131e-07, 8.431436606708131e-07, 0.00048565074854638835, 1.6862873213416263e-06, 1.6862873213416263e-06, 0.0001349029857073301, 2.107859151677033e-06, 2.107859151677033e-06, 0.003642380614097913, 5.058861964024879e-06, 5.058861964024879e-06, 0.00032376716569759224, 1.6862873213416263e-06, 1.6862873213416263e-06, 0.00024282537427319418, 1.2647154910062197e-06, 1.2647154910062197e-06, 0.0020235447856099516, 1.6862873213416263e-06, 1.6862873213416263e-06, 0.00032376716569759224, 1.6862873213416263e-06, 1.6862873213416263e-06, 0.0014569522456391651, 2.5294309820124393e-06, 2.5294309820124393e-06, 0.002185428368458748, 2.5294309820124393e-06, 2.5294309820124393e-06, 0.00016188358284879612, 8.431436606708131e-07, 8.431436606708131e-07, 0.00043168955426345633, 1.6862873213416263e-06, 1.6862873213416263e-06, 0.00032376716569759224, 1.2647154910062197e-06, 1.2647154910062197e-06, 0.0020235447856099516, 1.6862873213416263e-06, 1.6862873213416263e-06, 0.00043168955426345633, 1.6862873213416263e-06, 1.6862873213416263e-06, 0.0014569522456391651, 2.5294309820124393e-06, 2.5294309820124393e-06, 0.002185428368458748, 2.5294309820124393e-06, 2.5294309820124393e-06, 0.00043168955426345633, 1.6862873213416263e-06, 1.6862873213416263e-06, 0.00048565074854638835, 1.6862873213416263e-06, 1.6862873213416263e-06, 0.0003642380614097913, 1.2647154910062197e-06, 1.2647154910062197e-06, 0.0020235447856099516, 1.6862873213416263e-06, 1.6862873213416263e-06, 0.00048565074854638835, 1.6862873213416263e-06, 1.6862873213416263e-06, 0.0014569522456391651, 2.5294309820124393e-06, 2.5294309820124393e-06, 0.002185428368458748, 2.5294309820124393e-06, 2.5294309820124393e-06, 0.00048565074854638835, 1.6862873213416263e-06, 1.6862873213416263e-06, 0.026225140421504974, 1.0117723928049757e-05, 1.0117723928049757e-05, 0.00048565074854638835, 1.6862873213416263e-06, 1.6862873213416263e-06, 0.0014569522456391651, 2.5294309820124393e-06, 2.5294309820124393e-06, 0.002185428368458748, 2.5294309820124393e-06, 2.5294309820124393e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.002590137325580738, 3.3725746426832526e-06, 3.3725746426832526e-06, 0.0030218268798441945, 3.3725746426832526e-06, 3.3725746426832526e-06, 0.004532740319766291, 5.058861964024879e-06, 5.058861964024879e-06, 0.002590137325580738, 3.3725746426832526e-06, 3.3725746426832526e-06, 0.0030218268798441945, 3.3725746426832526e-06, 3.3725746426832526e-06, 0.0030218268798441945, 3.3725746426832526e-06, 3.3725746426832526e-06, 0.0030218268798441945, 3.3725746426832526e-06, 3.3725746426832526e-06, 0.004532740319766291, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.0032376716569759226, 4.215718303354066e-06, 4.215718303354066e-06, 0.004721604499756554, 4.215718303354066e-06, 4.215718303354066e-06, 0.0056659253997078645, 5.058861964024879e-06, 5.058861964024879e-06, 0.0032376716569759226, 4.215718303354066e-06, 4.215718303354066e-06, 0.004721604499756554, 4.215718303354066e-06, 4.215718303354066e-06, 0.004721604499756554, 4.215718303354066e-06, 4.215718303354066e-06, 0.004721604499756554, 4.215718303354066e-06, 4.215718303354066e-06, 0.0056659253997078645, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.0032376716569759226, 4.215718303354066e-06, 4.215718303354066e-06, 0.004721604499756554, 4.215718303354066e-06, 4.215718303354066e-06, 0.0056659253997078645, 5.058861964024879e-06, 5.058861964024879e-06, 0.0032376716569759226, 4.215718303354066e-06, 4.215718303354066e-06, 0.004721604499756554, 4.215718303354066e-06, 4.215718303354066e-06, 0.004721604499756554, 4.215718303354066e-06, 4.215718303354066e-06, 0.004721604499756554, 4.215718303354066e-06, 4.215718303354066e-06, 0.0056659253997078645, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.006799110479649437, 5.058861964024879e-06, 5.058861964024879e-06, 0.006799110479649437, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.006799110479649437, 5.058861964024879e-06, 5.058861964024879e-06, 0.006799110479649437, 5.058861964024879e-06, 5.058861964024879e-06, 0.006799110479649437, 5.058861964024879e-06, 5.058861964024879e-06, 0.006799110479649437, 5.058861964024879e-06, 5.058861964024879e-06, 0.003885205988371107, 5.058861964024879e-06, 5.058861964024879e-06, 0.002590137325580738, 3.3725746426832526e-06, 3.3725746426832526e-06, 0.06475343313951845, 2.0235447856099515e-05, 2.0235447856099515e-05, 0.020235447856099514, 2.634823939596291e-05]
    elif dnn == 'googlenet':
        t_f = [2.6239268637154135e-05, 1.1423899270597719e-05, 0.0003084452803061384, 3.4271697811793156e-05, 5.140754671768973e-05, 0.0003084452803061384, 8.567924452948289e-06, 1.2851886679422433e-05, 1.7135848905896578e-05, 9.139119416478175e-05, 9.139119416478175e-05, 0.0006168905606122768, 2.2847798541195438e-05, 7.71113200765346e-05, 4.5695597082390876e-05, 0.00025703773358844867, 0.00012851886679422433, 0.0005012235804974749, 2.1419811132370722e-05, 1.927783001913365e-05, 8.567924452948289e-05, 0.00022847798541195436, 0.00015993458978836805, 0.0006297424472916993, 3.4271697811793156e-05, 3.85556600382673e-05, 9.139119416478175e-05, 0.0001827823883295635, 0.0001827823883295635, 0.0008225207474830357, 3.4271697811793156e-05, 3.85556600382673e-05, 9.139119416478175e-05, 0.00015993458978836805, 0.00020563018687075893, 0.0010410028210332172, 4.5695597082390876e-05, 5.140754671768973e-05, 9.139119416478175e-05, 0.0003769886759297247, 0.00023561792245607794, 0.0012851886679422434, 4.712358449121559e-05, 0.00010281509343537947, 0.00018849433796486236, 0.0005940427620710814, 0.00037127672629442583, 0.0012851886679422434, 7.425534525888517e-05, 0.00010281509343537947, 0.0002970213810355407, 0.000891064143106622, 0.000445532071553311, 0.0018506716818368306, 0.00011138301788832776, 0.0001542226401530692, 0.0002970213810355407, 0.0028559748176494293, 2.789037907860771e-06]
    return t_f

def T_ar(nworkers, S_p, k):
    a, b = Get_a_and_b(nworkers, S_p)
    if k == 0:
        return float(0)
    elif k > 0:
        return float(a + b * S_p * k)

def Calculate_O_lay(L,P_1,m_re):
    layer = -1
    P = L+1
    for l in range(L - 1, -1, -1):
        if m_re[l] > 0 and P_1[l] < P:
            layer = l
            P = P_1[l]
    return layer

def Calculate_Delta_t(b,S_p,M,m,m_re,O_lay,i):
    if m_re[O_lay[i]] == 0:
        Delta_t = (m[O_lay[i]]-M[O_lay[i]]/S_p) * b * S_p
    else:
        Delta_t = 0
    return float(Delta_t)

def algorithm1(a,b,S_p,nworkers,L,M,P_1,t_b):
    m = np.full(L, 0)
    m_re = np.full(L, 0)
    for l in range(L):
        m[l] = math.ceil(M[l] / S_p)
        m_re[l] = 0

    tau_b = np.full(L, 0.5)
    tau_b[L-1] = 0.0
    for l in range(L - 2, -1, -1):
        tau_b[l] = tau_b[l + 1] + t_b[l + 1]

    tau_ar = []
    O_lay = []
    O_num = []
    tau_ar.append(tau_b[L-1]+t_b[L-1])
    i = 0
    L_old = -1
    for l in range(L - 1, -1, -1):
        while (tau_b[l]+t_b[l])-tau_ar[i] > 0:
            if Calculate_O_lay(L,P_1,m_re) != L_old:
                O_lay.append(Calculate_O_lay(L,P_1,m_re))
            else:
                i = i - 1
            if O_lay[i] == -1:
                O_lay.pop()
                tau_ar[i] = tau_b[l] + t_b[l]
                break
            if (tau_b[l] + t_b[l]) - tau_ar[i] < a:
                Delta_B = 1
            elif (tau_b[l] + t_b[l]) - tau_ar[i] >= a and (tau_b[l] + t_b[l]) - tau_ar[i] < T_ar(nworkers, S_p, m_re[O_lay[i]]):
                Delta_B = math.ceil(((tau_b[l] + t_b[l]) - tau_ar[i] - a) / (b * S_p))
            elif (tau_b[l] + t_b[l]) - tau_ar[i] >= T_ar(nworkers, S_p, m_re[O_lay[i]]):
                Delta_B = m_re[O_lay[i]]

            if O_lay[i] == L_old:
                O_num[i] = Delta_B
            else:
                O_num.append(Delta_B)
            m_re[O_lay[i]] = m_re[O_lay[i]] - O_num[i]
            Delta_t = Calculate_Delta_t(b,S_p,M,m,m_re,O_lay,i)
            if O_lay[i] == L_old:
                tau_ar[i+1] = tau_ar[i] + T_ar(nworkers, S_p, O_num[i]) - Delta_t
            else:
                tau_ar.append(tau_ar[i] + T_ar(nworkers, S_p, O_num[i]) - Delta_t)
            L_old = O_lay[i]
            i = i + 1
        m_re[l] = m[l]
    delta_1 = i
    return delta_1, O_num, O_lay, tau_ar, m_re, tau_b, t_b

def algorithm2(b, S_p, nworkers, L, M, P_2, tau_b, t_b, tau_ar, delta_1, delta_2, O_num, O_lay, m_re, t_f):
    m = np.full(L, 0)
    tau_f = np.full(L, 0.5)
    for l in range(L):
        m[l] = math.ceil(M[l] / S_p)
    Delta_t = Calculate_Delta_t(b, S_p, M, m, m_re, O_lay, delta_1)
    tau_ar[delta_1] = max(tau_b[0]+t_b[0], tau_ar[delta_1]) # tau_ar[delta_1]
    # print('tau_ar[delta_1-1]:', tau_ar[delta_1-1])
    # print('tau_ar[delta_1]:',tau_ar[delta_1])
    for i in range(delta_1+1, delta_1+delta_2, 1):
        Delta_t = Calculate_Delta_t(b, S_p, M, m, m_re, O_lay, i)
        tau_ar.append(tau_ar[i-1] + T_ar(nworkers, S_p, O_num[i-1]) - Delta_t) # tau_ar[i]
    for l in range(L):
        Delta_t = Calculate_Delta_t(b, S_p, M, m, m_re, O_lay, delta_1+P_2[l])
        if l == 0:
            tau_f[l] = tau_ar[delta_1+P_2[l]] + T_ar(nworkers, S_p, m_re[l]) - Delta_t
        else:
            tau_f[l] = max(tau_f[l-1]+t_f[l-1], tau_ar[delta_1+P_2[l]]+T_ar(nworkers, S_p, m_re[l])-Delta_t)
    return tau_f

def Calculate_t_iter(a, b, S_p, nworkers, L, M, P_1, t_b, P_2, t_f):
    delta_1, O_num, O_lay, tau_ar, m_re, tau_b, t_b = algorithm1(a, b, S_p, nworkers, L, M, P_1, t_b)
    # print(O_lay)
    # print(O_num)
    # print(tau_ar[delta_1])
    # print(delta_1)

    delta_2 = L
    for l in range(L):
        O_num.append(-1)
        O_lay.append(-1)
    for l in range(L):
        O_num[delta_1 + P_2[l]] = m_re[l]
        O_lay[delta_1 + P_2[l]] = l
    # print(O_lay)
    # print(O_num)

    tau_f = algorithm2(b, S_p, nworkers, L, M, P_2, tau_b, t_b, tau_ar, delta_1, delta_2, O_num, O_lay, m_re, t_f)

    return tau_f[L-1]+t_f[L-1]

def AdjustPriority(P_1, L, l, k):
    P = []
    for i in range(L):
        P.append(P_1[i])
    P[l] = P[k]
    for j in range(k, l, 1):
        P[j] = P[j] + 1
    return P

def algorithm3(a,b,S_p,nworkers,L,M,tau_b,t_b,t_f):
    m = np.full(L, 0)
    for l in range(L):
        m[l] = math.ceil(M[l] / S_p)

    P_1 = []
    P_2 = []
    for l in range(L):
        P_1.append(l)
        P_2.append(l)
    t_iter = Calculate_t_iter(a,b,S_p,nworkers,L,M,P_1,t_b,P_2,t_f)

    l = L-1
    while l >= 1:
        delta_1, O_num, O_lay, tau_ar, m_re, tau_b, t_b = algorithm1(a, b, S_p, nworkers, L, M, P_1, t_b)
        for i in range(delta_1):
            tau = 0
            if O_lay[i] == l:
                Delta_t = Calculate_Delta_t(b,S_p,M,m,m_re,O_lay,i)
                tau = tau_ar[i]+T_ar(nworkers, S_p, m[l])-Delta_t
                break
        if tau == 0:
            l = l - 1
            continue
        else:
            for k in range(l, -1, -1):
                if k == 0:
                    P_1_ast = AdjustPriority(P_1,L,l,0)
                elif tau_b[k]+t_b[k] < tau <= tau_b[k-1]+t_b[k-1]:
                    P_1_ast = AdjustPriority(P_1,L,l,k)
                    break
        t_iter_ast = Calculate_t_iter(a,b,S_p,nworkers,L,M,P_1_ast,t_b,P_2,t_f)
        if t_iter_ast < t_iter:
            t_iter = t_iter_ast
            for j in range(L):
                P_1[j] = P_1_ast[j]
            l = k

        l = l - 1
    return P_1, P_2

def US_Byte(a, b, S_p, nworkers, L, M, t_b, t_f):
    tau_b = np.full(L, 0.5)
    tau_b[L - 1] = 0.0
    for l in range(L - 2, -1, -1):
        tau_b[l] = tau_b[l + 1] + t_b[l + 1]

    P_1, P_2 = algorithm3(a, b, S_p, nworkers, L, M, tau_b, t_b, t_f)

    delta_1, O_num, O_lay, tau_ar, m_re, tau_b, t_b = algorithm1(a, b, S_p, nworkers, L, M, P_1, t_b)
    delta_2 = L
    for l in range(L):
        O_num.append(-1)
        O_lay.append(-1)
    for l in range(L):
        O_num[delta_1 + P_2[l]] = m_re[l]
        O_lay[delta_1 + P_2[l]] = l

    return delta_1, delta_2, O_lay, O_num

def Benchmarking_communication_performance():
    logger.info('Benchmarking communication performance...')
    comm_profiler = CommunicationProfiler(allreduce_async_, synchronize)
    sizes, times = comm_profiler.benchmark(num_iters=10)

    def _fit_linear_function(x, y):
        X = np.array(x).reshape((-1, 1)) * 4
        Y = np.array(y)
        model = LinearRegression()
        model.fit(X, Y)
        alpha = model.intercept_
        beta = model.coef_[0]
        return alpha, beta/4000000

    alpha, beta = _fit_linear_function(sizes, times)
    # self.alpha = alpha
    # self.beta = beta
    alpha_tensor = torch.ones(1) * alpha
    beta_tensor = torch.ones(1) * beta
    alpha_tensor = broadcast(alpha_tensor, root_rank=0)
    beta_tensor = broadcast(beta_tensor, root_rank=0)
    # if rank() != 0:
    #     self.alpha = float(alpha_tensor[0])
    #     self.beta = float(beta_tensor[0])
    logger.info('Communication performance fitted with f(p)=a+b*p, where a={} and b={}'.format(alpha, beta))
    res = 'alpha {:.10f} beta {:.10f}'.format(alpha, beta) + '\n'
    filename = './result/D-Credit-alpha_and_beta.txt'
    writeFile_add(filename, res)
    print("sizes:", sizes, '\n', 'times:', times, '\n')
    return alpha, beta

def run(dnn, dataset, data_dir, nworkers, lr, batch_size, nsteps_update, max_epochs, nwpernode, pretrain, num_steps, compression, partition):
    rank = hvd.rank()
    torch.cuda.set_device(rank%nwpernode)
    if rank != 0:
        pretrain = None
    trainer = DLTrainer(rank, nworkers, dist=False, batch_size=batch_size, is_weak_scaling=True, ngpus=1, data_dir=data_dir, dataset=dataset, dnn=dnn, lr=lr, nworkers=nworkers, prefix='allreduce', pretrain=pretrain, num_steps=num_steps, tb_writer=writer)

    init_epoch = torch.ones(1) * trainer.get_train_epoch()
    init_iter = torch.ones(1) * trainer.get_train_iter()
    trainer.set_train_epoch(int(hvd.broadcast(init_epoch, root_rank=0)[0]))
    trainer.set_train_iter(int(hvd.broadcast(init_iter, root_rank=0)[0]))

    seq_layernames, layerwise_times, layerwise_sizes = benchmark(trainer)
    layerwise_times = comm.bcast(layerwise_times, root=0)
    if rank == 0:
        logger.info('layerwise backward times: %s', list(layerwise_times))
        logger.info('layerwise backward sizes: %s', list(layerwise_sizes))
    logger.info('Bencharmked backward time: %f', np.sum(layerwise_times))
    logger.info('Model size: %d', np.sum(layerwise_sizes))

    # D-Credit
    model = trainer.net
    S_p = int(os.environ.get('BYTESCHEDULER_PARTITION', 1000000))
    a, b = Get_a_and_b(nworkers, S_p)
    L = len(list(model.named_parameters()))
    logger.info('L: %d', L)
    M = []
    t_b = []
    for l in range(L):
        M.append(float(layerwise_sizes[l]))
        t_b.append((float(layerwise_times[l])))

    t_f = Get_t_f(dnn)

    delta_1, delta_2, O_lay, O_num = US_Byte(a,b,S_p,nworkers,L,M,t_b,t_f)

    logger.info('delta_1: %d', delta_1)
    logger.info('delta_2: %d', delta_2)
    logger.info('O_lay: %s', list(O_lay))
    logger.info('O_num: %s', list(O_num))


    # bytescheduler wrapper
    use_bytescheduler_and_D_Credit = int(os.environ.get('USE_BYTESCHEDULER', '0'))
    if use_bytescheduler_and_D_Credit > 0:
        if partition:
            os.environ["BYTESCHEDULER_PARTITION"] = str(1000 * partition)
        import bytescheduler.pytorch.horovod as bsc
        bsc.init()

    optimizer = hvd.DistributedOptimizer(trainer.optimizer, named_parameters=trainer.net.named_parameters(), compression=compression)

    iters_per_epoch = trainer.get_num_of_training_samples() // (nworkers * batch_size * nsteps_update)
    if use_bytescheduler_and_D_Credit > 0:
        optimizer = bsc.ScheduledOptimizer(model, optimizer, max_epochs * iters_per_epoch, delta_1, delta_2, O_lay, O_num)

    hvd.broadcast_parameters(trainer.net.state_dict(), root_rank=0)
    hvd.broadcast_optimizer_state(optimizer, root_rank=0)
    trainer.update_optimizer(optimizer)

    times = []
    logger.info('max_epochs: %d', max_epochs)
    display = 10 if iters_per_epoch > 10 else iters_per_epoch-1

    display_speed = 20 if iters_per_epoch > 20 else iters_per_epoch - 1
    # accumulate_iters = 200
    img_secs = []

    for epoch in range(max_epochs):
        for i in range(iters_per_epoch):
            s = time.time()
            optimizer.zero_grad()
            for j in range(nsteps_update):
                if j < nsteps_update - 1 and nsteps_update > 1:
                    optimizer.local = True
                else:
                    optimizer.local = False
                trainer.train(1)
            trainer.update_model()
            times.append(time.time()-s)

            img_secs.append(batch_size * nsteps_update / (time.time()-s))
            display_speed -= 1
            if display_speed == 0:
                res = 'Total img/sec on {} GPU(s): {:.2f}'.format(nworkers, nworkers * np.mean(img_secs)) + '\n'
                filename = './result/partition_vs_speed'+'/US_Byte-' + dnn + '-' + str(nworkers) + '-' + str(S_p) + '.txt'
                writeFile_add(filename, res)
                img_secs = []
                display_speed = 20 if iters_per_epoch > 20 else iters_per_epoch - 1

            if i % display == 0 and i > 0:
                time_per_iter = np.mean(times)
                logger.warn('Time per iteration including communication: %f, Speed: %f images/s', time_per_iter, batch_size * nsteps_update / time_per_iter)
                times = []
            # accumulate_iters -= 1
            # if accumulate_iters == 0:
            #     os._exit(0)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="AllReduce trainer")
    parser.add_argument('--batch-size', type=int, default=32)
    parser.add_argument('--nsteps-update', type=int, default=1)
    parser.add_argument('--nworkers', type=int, default=1, help='Just for experiments, and it cannot be used in production')
    parser.add_argument('--nwpernode', type=int, default=1, help='Number of workers per node')
    parser.add_argument('--dataset', type=str, default='imagenet', choices=_support_datasets, help='Specify the dataset for training')
    parser.add_argument('--dnn', type=str, default='resnet50', choices=_support_dnns, help='Specify the neural network for training')
    parser.add_argument('--data-dir', type=str, default='./data', help='Specify the data root path')
    parser.add_argument('--saved-dir', type=str, default='.', help='Specify the saved weights or gradients root path')
    parser.add_argument('--lr', type=float, default=0.1, help='Default learning rate')
    parser.add_argument('--max-epochs', type=int, default=settings.MAX_EPOCHS, help='Default maximum epochs to train')
    parser.add_argument('--pretrain', type=str, default=None, help='Specify the pretrain path')
    parser.add_argument('--num-steps', type=int, default=35)
    parser.add_argument('--fp16-allreduce', action='store_true', default=False, help='use fp16 compression during allreduce')
    parser.add_argument('--partition', type=int, default=None, help='partition size')
    args = parser.parse_args()
    batch_size = args.batch_size * args.nsteps_update
    logdir = '%s-n%d-bs%d-lr%.4f' % (args.dnn, args.nworkers, batch_size, args.lr)
    relative_path = './logs/%s'%logdir
    gradient_relative_path = None 
    utils.create_path(relative_path)
    rank = 0
    if args.nworkers > 1:
        hvd.init()
        rank = hvd.rank()
    if rank == 0:
        tb_runs = './runs/%s'%logdir
        writer = None #SummaryWriter(tb_runs)
    logfile = os.path.join(relative_path, settings.hostname+'-'+str(rank)+'.log')
    hdlr = logging.FileHandler(logfile)
    hdlr.setFormatter(formatter)
    logger.addHandler(hdlr) 
    logger.info('Configurations: %s', args)

    # Benchmarking communication performance
    # a, b = Benchmarking_communication_performance()
    # Horovod: (optional) compression algorithm.
    compression = hvd.Compression.fp16 if args.fp16_allreduce else hvd.Compression.none

    run(args.dnn, args.dataset, args.data_dir, args.nworkers, args.lr, args.batch_size, args.nsteps_update, args.max_epochs, args.nwpernode, args.pretrain, args.num_steps, compression, args.partition)
